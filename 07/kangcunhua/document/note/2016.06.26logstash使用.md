[TOC]
#logstash 使用
##logstash三大组件
+ input
+ filter
+ output

### input
+ -e参数：直接输入输出指定事件，方便调试
    + logstash -e 'input { stdin {} } output { stdout {} }'
    + logstash -e  'input { stdin{} } output { stdout { codec=>rubydebug} }'
+ -f参数：正式使用指定配置文件的方式
    + ./bin/logstash -f conf.d/system.conf
    + 在当前用户home目录有.sincedb_*文件来记录每次读取配置文件指定的日志的偏移量
    + 测试:新增一条文档到日志,然后查看屏幕输出：注意用户权限
    + echo "reboot" >>/var/log/messages   

```shell
{
       "message" => "Jul 18 12:34:54 bogon kernel: e1000: eth1 NIC Link is Down",
      "@version" => "1",
    "@timestamp" => "2016-07-18T07:55:09.707Z",
          "path" => "/var/log/messages",
          "host" => "0.0.0.0",
          "type" => "system"
}
{
       "message" => "Jul 18 12:55:37 bogon kernel: e1000: eth1 NIC Link is Up 1000 Mbps Full Duplex, Flow Control: RX",
      "@version" => "1",
    "@timestamp" => "2016-07-18T07:55:09.707Z",
          "path" => "/var/log/messages",
          "host" => "0.0.0.0",
          "type" => "system"
}
{
       "message" => "reboot",
      "@version" => "1",
    "@timestamp" => "2016-07-18T07:55:09.708Z",
          "path" => "/var/log/messages",
          "host" => "0.0.0.0",
          "type" => "system"
}
```

+ -t参数：测试配置文件能否正常解析


```shell
[vagrant@bogon logstash-2.2.0]$ ./bin/logstash -f ./conf.d/system.conf -t
Configuration OK
```

+ Redis连接shipper和index的配置文件例子：
    + [shipper.conf](conf.d/shipper.conf)
    + [index.conf](conf.d/index.conf)

### filter
主要是正则表达式匹配相关,非常耗费资源性能
http://grokdebug.herokuapp.com/patterns#
对不规则的日志进行匹配生产json：如果日志本身就是json的就省事了

### output
#### 标准输出：
可以指定输出到多个目标
####elasticsearch: 发送事件数据到 Elasticsearch。
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                
```shell
output {
  elasticsearch {
    hosts => ["192.168.0.2:9200"]      # 或者cluster => “ClusterName“
    index => "logstash-%{type}-%{+YYYY.MM.dd}"     #索引名，统一格式，方便kibana导入，会讲统一类型的日志，全部导入
    document_type => "nginx"
    workers => 1                          #启动一个进程
    flush_size => 20000              # 攒够20000 条数据一次性发给ES，默认500条
    idle_flush_time => 10          # 如果10s内没攒够 20000 条也发一次给ES,默认1s
    template_overwrite => true
  }
}
```

## ngnix日志格式配置

+ 默认
```shell
log_format access '$remote_addr - $remote_user [$time_local] "srequest" '
                  '$status $body_bytes_sent "$http_referer" '
                  '$http_user_agent" "$http_x_forwarded_for" ';
```
+ 推荐
```shell
log_format json '{"@timestamp":"$time_iso8601",'
                '"@version":"1",'
                '"host":"$server_addr",'
                '"client":"$remote_addr",'
                '"size":"$body_bytes_sent",'
                '"responsetime":"$request_time",'
                '"domain":"$host",'
                '"url":"$uri",'
                '"refer":"$http_referer",'
                '"agent":"$http_user_agent",'
                '"status":"$status",'
```

### 分析ngnix日志demo
Logstash 指定配置文件处理日志，发送到ES
 ./bin/logstash -f ./conf.d/ngnix.conf
在kibana查看并搜索404相关日志
```shell
http://192.168.99.20:5601/app/kibana#/discover?_g=(refreshInterval:(display:Off,pause:!f,value:0),time:(from:'2016-06-24T16:00:00.000Z',mode:absolute,to:'2016-06-25T16:00:00.000Z'))&_a=(columns:!(_source),index:'logstash-*',interval:auto,query:(query_string:(analyze_wildcard:!t,query:'404%20AND%2012')),sort:!('@timestamp',desc))
```
kibana![kibana](document/screenshots/20160718222404_logstash_ngnix_es_kibana.png)

##input插件之TCP写入

+ 安装nc工具

```shell
sudo yum install -y nc
```

+ 启动 Logstash
    + ./bin/logstash -f ./conf.d/tcp.conf
+ 查看端口
+ netstat -nulpt | grep 8888
+ 发送数据
    + nc 127.0.0.1 8888 < access.json                                                 # 可以是json文件直接传值
    + echo '{"name":"mandaren","age":"18"}' |nc 127.0.0.1 8888           #  也可以是命令实时传值

```shell
[vagrant@bogon ~]$ netstat -nulpt |grep 8888
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
tcp        0      0 :::8888                     :::*                        LISTEN      9333/java           
[vagrant@bogon ~]$ echo '{"name":"mandaren","age":"18"}' |nc 127.0.0.1 8888
[vagrant@bogon ~]$ 
```

+ Logstash的启动和输出
```shell
[vagrant@bogon logstash-2.2.0]$ ./bin/logstash -f ./conf.d/tcp.conf -t
Configuration OK
[vagrant@bogon logstash-2.2.0]$ ./bin/logstash -f ./conf.d/tcp.conf
Settings: Default pipeline workers: 1
Logstash startup completed
{
          "name" => "mandaren",
           "age" => "18",
      "@version" => "1",
    "@timestamp" => "2016-07-19T02:06:45.852Z",
          "host" => "127.0.0.1",
          "port" => 45083
}
```


## logstash命令行参数
http://blog.csdn.net/u010454030/article/details/49659467
进入根目录，执行bin/logstash -h 可查看帮助文档 
参数介绍如下： 

###使用命令模板: 
+ /bin/logstash 命令参数  选项 

###选项: 
+ -f ， 指定加载一个后缀为.conf文件的logstash配置模块 
+ -e  , 命令行指定参数 ， 通常用来调试 
+ -w,  指定logstash的工作线程数 
+ -l,   指定logstash的默认日志写入到一个文件中，如果不指定，默认是标准输出 
+ --quiet                       静默模式，仅仅只有error级别信息输出 
+ --verbose                   info级别的log输出 
+ --debug                      debug 级别的log输出. 
+ -V, --version                查看logstash的版本 
+ -p, --pluginpath PATH         加载自定义的logstash插件 
+ -t, --configtest               检查logstash配置是否有效 
+ -h, --help                    打印帮助 
